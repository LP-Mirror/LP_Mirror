# LP-Mirror: Data Selection Framework for Time Series Forecasting

# LP-Mirror: æ—¶é—´åºåˆ—é¢„æµ‹æ•°æ®ç­›é€‰æ¡†æ¶

**LP-Mirror** is a data selection method designed for time series forecasting tasks. It filters training batches to identify and retain high-quality samples. The code is compatible with mainstream time series datasets and forecasting frameworks.
**LP-Mirror** æ˜¯ä¸€ä¸ªä¸“ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡çš„æ•°æ®ç­›é€‰æ–¹æ³•ã€‚å®ƒé€šè¿‡ç­›é€‰è®­ç»ƒæ‰¹æ¬¡æ¥è¯†åˆ«å¹¶ä¿ç•™é«˜è´¨é‡æ ·æœ¬ã€‚è¯¥ä»£ç é€‚é…ä¸»æµæ—¶é—´åºåˆ—æ•°æ®ï¼Œå¯ç›´æ¥ç”¨äºç°æœ‰çš„é¢„æµ‹æ¡†æ¶ä¸­ã€‚

You can customize and run the shell scripts to execute the selection process. Please refer to `run_selection.py` or `run_cross_selection.py` for implementation details.
æ‚¨å¯ä»¥ä¿®æ”¹å¹¶è¿è¡Œ shell è„šæœ¬æ¥æ‰§è¡Œç­›é€‰è¿‡ç¨‹ï¼Œå…·ä½“å®ç°ç»†èŠ‚è¯·å‚è€ƒ `run_selection.py` or `run_cross_selection.py`ã€‚

> âš ï¸ **Note / æ³¨æ„**
> Since the effective dataset size will decrease after selection, **hyperparameter tuning (especially learning rate or batch size) is necessary** to prevent overfitting or underfitting.
> ç”±äºç­›é€‰åæœ‰æ•ˆæ•°æ®é‡ä¼šå‡å°‘ï¼Œ**å¯¹æ¨¡å‹è¿›è¡Œé€‚å½“çš„å‚æ•°è°ƒæ•´ï¼ˆç‰¹åˆ«æ˜¯å­¦ä¹ ç‡æˆ– Batch Sizeï¼‰æ˜¯å¿…è¦çš„**ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆã€‚

---

## ğŸ› ï¸ Integration Guide / é›†æˆæŒ‡å—

This guide explains how to modify the source code of mainstream time series forecasting frameworks (e.g., **Time-Series-Library**, **Autoformer**, **Informer**) to support loading high-quality data indices (`.npy` files) generated by LP-Mirror.
æœ¬æŒ‡å—è¯´æ˜äº†å¦‚ä½•ä¿®æ”¹ä¸»æµæ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ï¼ˆå¦‚ **Time-Series-Library**, **Autoformer** ç­‰ï¼‰çš„æºç ï¼Œä»¥æ”¯æŒåŠ è½½ç”± **LP-Mirror ç®—æ³•**ç­›é€‰å‡ºçš„é«˜è´¨é‡æ•°æ®ç´¢å¼•ï¼ˆ`.npy` æ–‡ä»¶ï¼‰ã€‚

By following these steps, you can train your model using only the "effective samples" while keeping the original data preprocessing logic (e.g., Standardization) intact.
é€šè¿‡ä»¥ä¸‹ä¿®æ”¹ï¼Œæ‚¨å¯ä»¥åœ¨ä¿æŒåŸå§‹æ•°æ®é¢„å¤„ç†é€»è¾‘ï¼ˆå¦‚æ ‡å‡†åŒ–ï¼‰ä¸å˜çš„å‰æä¸‹ï¼Œä»…ä½¿ç”¨ç­›é€‰åçš„â€œæœ‰æ•ˆæ ·æœ¬â€è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

### Step 1: Modify `run_longExp.py` / ä¿®æ”¹ä¸»è¿è¡Œè„šæœ¬

Add a new command-line argument to receive the path of the selection result file.
æˆ‘ä»¬éœ€è¦åœ¨ä¸»è¿è¡Œè„šæœ¬ä¸­æ·»åŠ ä¸€ä¸ªæ–°çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæ¥æ”¶ç­›é€‰ç»“æœæ–‡ä»¶çš„è·¯å¾„ã€‚

**File / æ–‡ä»¶:** `./run_longExp.py`

Locate the argument definition section (usually under `if __name__ == '__main__':`) and add the `--data_selection_path` argument:
æ‰¾åˆ°å‚æ•°å®šä¹‰éƒ¨åˆ†ï¼ˆé€šå¸¸åœ¨ `if __name__ == '__main__':` ä¸‹ï¼‰ï¼Œæ·»åŠ  `--data_selection_path` å‚æ•°ï¼š

```python
import argparse
# ... other imports ...

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')

    # ... existing arguments (e.g., --root_path, --data_path) ...
    parser.add_argument('--patience', type=int, default=3, help='early stopping patience')
    parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')

    # ================= [Added Code / æ–°å¢ä»£ç ] =================
    # Path to the .npy file containing selected indices. Default is None (use full data).
    # æ·»åŠ æ•°æ®ç­›é€‰è·¯å¾„å‚æ•°ï¼Œé»˜è®¤ä¸º Noneï¼ˆå³ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
    parser.add_argument('--data_selection_path', type=str, default=None, help='Path to the .npy file containing selected indices')
    # =========================================================

    args = parser.parse_args()
    # ... following code ...

```

### Step 2: Modify `data_factory.py` / ä¿®æ”¹æ•°æ®å·¥å‚

This is the core modification. We need to intercept the dataset after instantiation and prune it using the `Subset` class based on the indices.
è¿™æ˜¯æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†ã€‚æˆ‘ä»¬éœ€è¦åœ¨æ•°æ®åŠ è½½å™¨æ„å»ºæ•°æ®æ—¶ï¼Œæ‹¦æˆªè®­ç»ƒé›†ï¼Œå¹¶åˆ©ç”¨ `Subset` ç±»æ ¹æ®ç´¢å¼•è¿›è¡Œè£å‰ªã€‚

**File / æ–‡ä»¶:** `./data_provider/data_factory.py`

#### 2.1 Add Imports / æ·»åŠ å¯¼å…¥

Add the following imports at the top of the file:
åœ¨æ–‡ä»¶å¤´éƒ¨æ·»åŠ ä»¥ä¸‹å¯¼å…¥ï¼š

```python
from data_loader import Dataset_ETT_hour, Dataset_ETT_minute, Dataset_Custom, Dataset_Pred
from torch.utils.data import DataLoader

# ================= [Added Imports / æ–°å¢å¯¼å…¥] =================
from torch.utils.data import Subset  # For creating data subsets / ç”¨äºåˆ›å»ºæ•°æ®å­é›†
import numpy as np                   # For loading .npy files / ç”¨äºåŠ è½½ .npy æ–‡ä»¶
import os                            # For checking path existence / ç”¨äºæ£€æŸ¥è·¯å¾„å­˜åœ¨æ€§
# ============================================================

```

#### 2.2 Modify Logic in `data_provider` / ä¿®æ”¹ `data_provider` é€»è¾‘

In the `data_provider` function, add the filtering logic **after** the `Dataset` is instantiated:
åœ¨ `data_provider` å‡½æ•°ä¸­ï¼Œåœ¨å®ä¾‹åŒ– `Dataset` **ä¹‹å**ï¼Œæ·»åŠ ç­›é€‰é€»è¾‘ï¼š

```python
def data_provider(args, flag):
    Data = data_dict[args.data]
    # ... (omit timeenc setup) ...
    # ... (omit shuffle_flag, batch_size setup) ...

    # 1. Instantiate the dataset normally
    # Note: It is crucial to fit the StandardScaler on the FULL dataset first to ensure consistency.
    # æ³¨æ„ï¼šè¿™ä¸€æ­¥éå¸¸é‡è¦ï¼å¿…é¡»å…ˆå®ä¾‹åŒ–å®Œæ•´æ•°æ®é›†ï¼Œç¡®ä¿ StandardScaler æ˜¯åœ¨å®Œæ•´æ•°æ®ä¸Š fit çš„ã€‚
    data_set = Data(
        root_path=args.root_path,
        data_path=args.data_path,
        flag=flag,
        size=[args.seq_len, args.label_len, args.pred_len],
        features=args.features,
        target=args.target,
        timeenc=timeenc,
        freq=freq
    )

    # ================= [Added Logic: Data Selection / æ–°å¢ä»£ç : æ•°æ®ç­›é€‰é€»è¾‘] =================
    # Execute only in 'train' mode and when a selection path is provided
    # ä»…åœ¨ 'train' æ¨¡å¼ä¸”ä¼ å…¥äº†ç­›é€‰è·¯å¾„æ—¶æ‰§è¡Œ
    if flag == 'train' and hasattr(args, 'data_selection_path') and args.data_selection_path is not None:
        if os.path.exists(args.data_selection_path):
            print(f"\n>>>>>> [LP-Mirror] Loading selected data indices from: {args.data_selection_path}")
            try:
                # 1. Load selected indices / åŠ è½½ç­›é€‰å‡ºçš„ç´¢å¼•
                selected_indices = np.load(args.data_selection_path)
                
                # 2. Record original size / è®°å½•åŸå§‹æ•°æ®é‡
                original_len = len(data_set)
                
                # 3. Apply Subset to prune data / ä½¿ç”¨ Subset è¿›è¡Œè£å‰ª
                data_set = Subset(data_set, selected_indices)
                
                print(f">>>>>> [LP-Mirror] Selection Applied.")
                print(f"       Original Size: {original_len} -> Selected Size: {len(data_set)}")
                print(f"       Retention Ratio: {len(data_set)/original_len*100:.2f}%\n")
            except Exception as e:
                print(f">>>>>> [Error] Failed to load indices: {e}. Training on FULL dataset.")
        else:
            print(f"\n>>>>>> [Warning] Path not found: {args.data_selection_path}. Training on FULL dataset.\n")
    # ====================================================================================

    # Create DataLoader normally
    data_loader = DataLoader(
        data_set,
        batch_size=batch_size,
        # ... existing params ...
    )
    return data_set, data_loader

```

---

## ğŸš€ Usage / ä½¿ç”¨æ–¹æ³•

After completing the modifications above, follow these steps to run your experiment:
å®Œæˆä¸Šè¿°ä¿®æ”¹åï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æµç¨‹è¿›è¡Œå®éªŒï¼š

### Step 1: Run Selection Script / è¿è¡Œç­›é€‰è„šæœ¬

Run your selection script (e.g., `run_selection.py`) to generate the index file.
å…ˆè¿è¡Œæ‚¨çš„ç­›é€‰è„šæœ¬ï¼ˆä¾‹å¦‚ `run_selection.py`ï¼‰ï¼Œç”Ÿæˆç´¢å¼•æ–‡ä»¶ã€‚

```bash
python run_selection.py --data_path ETTh1.csv --save_path ./results/selection_ETTh1/
# Output: ./results/selection_ETTh1/selected_indices.npy

```

### Step 2: Run Main Training Script / è¿è¡Œä¸»è®­ç»ƒè„šæœ¬

When running `run_longExp.py`, point to the generated `.npy` file using the `--data_selection_path` argument.
åœ¨è¿è¡Œ `run_longExp.py` æ—¶ï¼Œé€šè¿‡ `--data_selection_path` å‚æ•°æŒ‡å‘åˆšæ‰ç”Ÿæˆçš„ `.npy` æ–‡ä»¶ã€‚

**Example / ç¤ºä¾‹å‘½ä»¤:**

```bash
python -u run_longExp.py \
  --is_training 1 \
  --root_path ./dataset/ \
  --data_path ETTh1.csv \
  --model_id ETTh1_96_96 \
  --model TimesNet \
  --data ETTh1 \
  --features M \
  --seq_len 96 \
  --label_len 48 \
  --pred_len 96 \
  --data_selection_path ./results/selection_ETTh1/selected_indices.npy

```

### Result Verification / ç»“æœéªŒè¯

If integrated successfully, you will see output similar to the following in your console:
å¦‚æœé›†æˆæˆåŠŸï¼Œæ‚¨å°†åœ¨æ§åˆ¶å°è¾“å‡ºä¸­çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„ä¿¡æ¯ï¼š

```text
>>>>>> [LP-Mirror] Loading selected data indices from: ./results/selection_ETTh1/selected_indices.npy
>>>>>> [LP-Mirror] Selection Applied.
       Original Size: 8545 -> Selected Size: 5981
       Retention Ratio: 70.00%

```

This indicates that the model is now training exclusively on the "High Quality" data verified by LP-Mirror.
è¿™æ„å‘³ç€æ¨¡å‹ç°åœ¨åªä½¿ç”¨ç»è¿‡ LP-Mirror ç®—æ³•éªŒè¯çš„â€œé«˜è´¨é‡â€æ•°æ®è¿›è¡Œè®­ç»ƒã€‚